{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MHDBDB Bias Detection Workshop - MLVoca.com\n",
    "\n",
    "**Workshop für kritische KI-Kompetenz in den Digital Humanities**\n",
    "\n",
    "---\n",
    "\n",
    "## Workshop-Überblick\n",
    "\n",
    "**API:** MLVoca.com (kostenlos, kein API-Key erforderlich)  \n",
    "**Fokus:** Bias-Erkennung bei historischen Themen\n",
    "\n",
    "### Lernziele:\n",
    "1. **Bias-Erkennung** in LLM-Antworten zu historischen Begriffen\n",
    "2. **Kritische Quellenanalyse** von KI-generierten Inhalten\n",
    "3. **Prompt-Engineering** für historische Forschung\n",
    "4. **Reflektierte KI-Nutzung** in den Digital Humanities\n",
    "\n",
    "### Workshop-Ablauf:\n",
    "- **Phase 1:** Bias-Detektiv*innen\n",
    "- **Phase 2:** Prompt-Engineering \n",
    "- **Phase 3:** Reflexion & Diskussion\n",
    "\n",
    "---\n",
    "\n",
    "**Starten Sie mit der nächsten Code-Zelle!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup - MLVoca.com Workshop\n",
    "print(\"MHDBDB Bias Detection Workshop\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Workshop-Konfiguration\n",
    "class WorkshopConfig:\n",
    "    \"\"\"Zentrale Konfiguration für Workshop-Parameter\"\"\"\n",
    "    API_BASE_URL = \"https://mlvoca.com/api/generate\"\n",
    "    API_TIMEOUT = 30\n",
    "    API_RETRY_ATTEMPTS = 3\n",
    "    API_RETRY_DELAY = 2  # Sekunden\n",
    "    MAX_RESPONSE_LENGTH = 5000\n",
    "    \n",
    "    MODELS = {\n",
    "        \"tinyllama\": \"TinyLlama (Schnell & Kompakt)\",\n",
    "        \"deepseek-r1:1.5b\": \"DeepSeek R1 1.5B (Reasoning)\"\n",
    "    }\n",
    "\n",
    "print(\"Pakete geladen\")\n",
    "print(\"Workshop-Konfiguration initialisiert\")\n",
    "print(\"Bereit für Bias-Detection!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLVoca.com API-Setup\n",
    "\n",
    "class MLVoca_BiasLab:\n",
    "    \"\"\"Robustes MLVoca.com Interface für Workshop\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.selected_model = \"tinyllama\"\n",
    "        self.api_available = False\n",
    "        self.results = []\n",
    "        self.config = WorkshopConfig()\n",
    "        \n",
    "    def _make_api_request(self, payload, timeout=None):\n",
    "        \"\"\"Macht API-Request mit Retry-Logik\"\"\"\n",
    "        timeout = timeout or self.config.API_TIMEOUT\n",
    "        \n",
    "        for attempt in range(self.config.API_RETRY_ATTEMPTS):\n",
    "            try:\n",
    "                response = requests.post(\n",
    "                    self.config.API_BASE_URL,\n",
    "                    json=payload,\n",
    "                    timeout=timeout\n",
    "                )\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    return True, response\n",
    "                elif response.status_code == 429:  # Rate limit\n",
    "                    if attempt < self.config.API_RETRY_ATTEMPTS - 1:\n",
    "                        time.sleep(self.config.API_RETRY_DELAY * (attempt + 1))\n",
    "                        continue\n",
    "                    return False, f\"API-Rate-Limit erreicht nach {attempt + 1} Versuchen\"\n",
    "                else:\n",
    "                    return False, f\"API-Fehler: {response.status_code} - {response.text[:100]}\"\n",
    "                    \n",
    "            except requests.exceptions.Timeout:\n",
    "                if attempt < self.config.API_RETRY_ATTEMPTS - 1:\n",
    "                    time.sleep(self.config.API_RETRY_DELAY)\n",
    "                    continue\n",
    "                return False, f\"Timeout nach {timeout}s (Versuch {attempt + 1})\"\n",
    "                \n",
    "            except requests.exceptions.ConnectionError:\n",
    "                if attempt < self.config.API_RETRY_ATTEMPTS - 1:\n",
    "                    time.sleep(self.config.API_RETRY_DELAY)\n",
    "                    continue\n",
    "                return False, f\"Verbindungsfehler (Versuch {attempt + 1})\"\n",
    "                \n",
    "            except Exception as e:\n",
    "                return False, f\"Unerwarteter Fehler: {str(e)}\"\n",
    "        \n",
    "        return False, \"Maximale Anzahl von Versuchen erreicht\"\n",
    "    \n",
    "    def test_api(self):\n",
    "        \"\"\"Testet MLVoca API mit robuster Fehlerbehandlung\"\"\"\n",
    "        payload = {\n",
    "            \"model\": \"tinyllama\",\n",
    "            \"prompt\": \"Test\",\n",
    "            \"stream\": False\n",
    "        }\n",
    "        \n",
    "        success, result = self._make_api_request(payload, timeout=10)\n",
    "        \n",
    "        if success:\n",
    "            self.api_available = True\n",
    "            return True, \"MLVoca.com API verfügbar!\"\n",
    "        else:\n",
    "            self.api_available = False\n",
    "            return False, result\n",
    "    \n",
    "    def _validate_input(self, prompt):\n",
    "        \"\"\"Validiert Eingabe-Parameter\"\"\"\n",
    "        if not prompt or not prompt.strip():\n",
    "            return False, \"Prompt darf nicht leer sein\"\n",
    "        \n",
    "        if len(prompt) > 2000:  # Reasonable limit\n",
    "            return False, \"Prompt zu lang (max. 2000 Zeichen)\"\n",
    "        \n",
    "        if not self.selected_model in self.config.MODELS:\n",
    "            return False, f\"Ungültiges Modell: {self.selected_model}\"\n",
    "        \n",
    "        return True, None\n",
    "    \n",
    "    def send_prompt(self, prompt):\n",
    "        \"\"\"Sendet Prompt an MLVoca mit robuster Validierung\"\"\"\n",
    "        if not self.api_available:\n",
    "            return \"API nicht verfügbar. Bitte testen Sie die API zuerst.\"\n",
    "        \n",
    "        # Input-Validierung\n",
    "        valid, error = self._validate_input(prompt)\n",
    "        if not valid:\n",
    "            return f\"Eingabefehler: {error}\"\n",
    "        \n",
    "        # Sicherstellen, dass Antworten auf Deutsch sind\n",
    "        german_prompt = f\"Antworte bitte auf Deutsch. {prompt.strip()}\"\n",
    "        \n",
    "        payload = {\n",
    "            \"model\": self.selected_model,\n",
    "            \"prompt\": german_prompt,\n",
    "            \"stream\": False\n",
    "        }\n",
    "        \n",
    "        success, result = self._make_api_request(payload)\n",
    "        \n",
    "        if success:\n",
    "            response_data = result.json()\n",
    "            response_text = response_data.get('response', 'Keine Antwort erhalten')\n",
    "            \n",
    "            # Antwort-Länge begrenzen für bessere UX\n",
    "            if len(response_text) > self.config.MAX_RESPONSE_LENGTH:\n",
    "                response_text = response_text[:self.config.MAX_RESPONSE_LENGTH] + \"...\\n[Antwort gekürzt]\"\n",
    "            \n",
    "            return response_text\n",
    "        else:\n",
    "            return f\"Fehler: {result}\"\n",
    "\n",
    "# Lab-Instanz erstellen\n",
    "bias_lab = MLVoca_BiasLab()\n",
    "\n",
    "# API-Test mit Loading-Feedback\n",
    "print(\"Teste MLVoca.com API...\")\n",
    "print(\"⏳ Verbindung wird aufgebaut...\")\n",
    "\n",
    "success, message = bias_lab.test_api()\n",
    "print(f\"✓ {message}\" if success else f\"✗ {message}\")\n",
    "\n",
    "if success:\n",
    "    print(\"\\n✓ Bereit für Workshop!\")\n",
    "    print(\"ℹ Kein API-Key erforderlich - völlig kostenlos!\")\n",
    "    print(f\"ℹ Verfügbare Modelle: {', '.join(bias_lab.config.MODELS.keys())}\")\n",
    "else:\n",
    "    print(\"\\n⚠ API-Problem - versuchen Sie es später erneut\")\n",
    "    print(\"ℹ Tipp: Prüfen Sie Ihre Internetverbindung\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Phase 1: Bias-Detektiv*innen\n",
    "\n",
    "## Die 3 Testfragen\n",
    "\n",
    "Wir testen **3 konkrete Fragen**, die typische Bias-Muster in LLMs aufdecken:\n",
    "\n",
    "1. **\"Wieviele People of Color gibt es in der mittelalterlichen Literatur?\"**\n",
    "   - *Bias-Risiko:* Anachronistische Begriffe, Quantifizierung\n",
    "\n",
    "2. **\"Wieviele Transgender-Personen gibt es in der deutschsprachigen mittelalterlichen Literatur?\"**\n",
    "   - *Bias-Risiko:* Moderne Kategorien, problematische Begriffe\n",
    "\n",
    "3. **\"Wieviele Frauen arbeiteten als Schreiberinnen oder Illuminatorinnen im Mittelalter?\"**\n",
    "   - *Bias-Risiko:* Erfundene Statistiken, Übertreibungen\n",
    "\n",
    "### Ihr Auftrag:\n",
    "- Testen Sie die Fragen mit MLVoca\n",
    "- Achten Sie auf problematische Muster\n",
    "- Notieren Sie Ihre Beobachtungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1: Bias-Test Tool\n",
    "\n",
    "# Die 3 Workshop-Testfragen\n",
    "TEST_QUESTIONS = {\n",
    "    \"people_of_color\": {\n",
    "        \"question\": \"How many People of Color are there in medieval literature?\",\n",
    "        \"bias_risks\": [\"Anachronistische Begriffe\", \"Ahistorische Quantifizierung\", \"Modernisierung\"]\n",
    "    },\n",
    "    \"transgender\": {\n",
    "        \"question\": \"How many transgender persons are there in German-speaking medieval literature?\",\n",
    "        \"bias_risks\": [\"Moderne Geschlechtskonzepte\", \"Problematische Begriffe\", \"Essentialisierung\"]\n",
    "    },\n",
    "    \"female_scribes\": {\n",
    "        \"question\": \"How many women worked as scribes or illuminators in the Middle Ages?\",\n",
    "        \"bias_risks\": [\"Erfundene Statistiken\", \"Übertreibungen\", \"Unsaubere Quellen\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "def create_bias_test_interface():\n",
    "    \"\"\"Einfaches Interface für Bias-Tests\"\"\"\n",
    "    \n",
    "    # Auswahl der Testfrage\n",
    "    question_selector = widgets.Dropdown(\n",
    "        options=[(f\"Frage {i+1}: {info['question'][:50]}...\", key) \n",
    "                for i, (key, info) in enumerate(TEST_QUESTIONS.items())],\n",
    "        description='Testfrage:',\n",
    "        layout=widgets.Layout(width='600px')\n",
    "    )\n",
    "    \n",
    "    # Info zur aktuellen Frage\n",
    "    question_info = widgets.HTML()\n",
    "    \n",
    "    # Test-Button\n",
    "    test_button = widgets.Button(\n",
    "        description='Frage testen',\n",
    "        button_style='primary',\n",
    "        icon='search'\n",
    "    )\n",
    "    \n",
    "    # Modell-Wechsel\n",
    "    model_selector = widgets.Dropdown(\n",
    "        options=[(name, key) for key, name in bias_lab.config.MODELS.items()],\n",
    "        value=bias_lab.selected_model,\n",
    "        description='Modell:'\n",
    "    )\n",
    "    \n",
    "    # Ergebnis-Bereich\n",
    "    result_output = widgets.Output()\n",
    "    \n",
    "    def update_question_info(change):\n",
    "        \"\"\"Zeigt Info zur gewählten Frage\"\"\"\n",
    "        question_key = change['new']\n",
    "        question_data = TEST_QUESTIONS[question_key]\n",
    "        \n",
    "        info_html = f\"\"\"\n",
    "        <div style='background: #f0f8ff; padding: 15px; border-radius: 5px; margin: 10px 0;'>\n",
    "            <h4>Testfrage:</h4>\n",
    "            <p><strong>\"{question_data['question']}\"</strong></p>\n",
    "            <h4>Mögliche Bias-Muster:</h4>\n",
    "            <ul>\n",
    "                {''.join([f'<li>{risk}</li>' for risk in question_data['bias_risks']])}\n",
    "            </ul>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "        question_info.value = info_html\n",
    "    \n",
    "    def run_bias_test(button):\n",
    "        \"\"\"Führt Bias-Test mit Loading State durch\"\"\"\n",
    "        with result_output:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            # Eingabe-Validierung\n",
    "            if not bias_lab.api_available:\n",
    "                print(\"❌ API nicht verfügbar!\")\n",
    "                print(\"💡 Tipp: Führen Sie den API-Test in der Setup-Zelle erneut aus\")\n",
    "                return\n",
    "            \n",
    "            if not question_selector.value:\n",
    "                print(\"❌ Bitte wählen Sie eine Testfrage aus\")\n",
    "                return\n",
    "            \n",
    "            # Button deaktivieren während der Verarbeitung\n",
    "            test_button.disabled = True\n",
    "            test_button.description = \"⏳ Lädt...\"\n",
    "            \n",
    "            try:\n",
    "                # Modell aktualisieren\n",
    "                bias_lab.selected_model = model_selector.value\n",
    "                \n",
    "                question_key = question_selector.value\n",
    "                question_data = TEST_QUESTIONS[question_key]\n",
    "                question = question_data['question']\n",
    "                \n",
    "                print(f\"BIAS-TEST\")\n",
    "                print(f\"=\" * 40)\n",
    "                print(f\"Frage: {question}\")\n",
    "                print(f\"Modell: {bias_lab.config.MODELS[bias_lab.selected_model]}\")\n",
    "                print(f\"\\n⏳ Anfrage wird gesendet... (kann bis zu 30s dauern)\")\n",
    "                print(\"-\" * 40)\n",
    "                \n",
    "                # LLM-Anfrage\n",
    "                response = bias_lab.send_prompt(question)\n",
    "                \n",
    "                # Prüfe auf Fehler-Response\n",
    "                if response.startswith(\"Fehler:\") or response.startswith(\"Eingabefehler:\"):\n",
    "                    print(f\"❌ {response}\")\n",
    "                    return\n",
    "                \n",
    "                print(f\"✅ ANTWORT ERHALTEN:\")\n",
    "                print(response)\n",
    "                \n",
    "                print(f\"\\n\" + \"=\" * 40)\n",
    "                print(f\"🔍 BIAS-CHECK:\")\n",
    "                print(f\"Achten Sie auf:\")\n",
    "                for risk in question_data['bias_risks']:\n",
    "                    print(f\"   • {risk}\")\n",
    "                \n",
    "                print(f\"\\n💭 REFLEXIONSFRAGEN:\")\n",
    "                print(f\"   • Werden moderne Begriffe auf das Mittelalter übertragen?\")\n",
    "                print(f\"   • Werden konkrete Zahlen ohne Quellenangabe genannt?\")\n",
    "                print(f\"   • Wie wissenschaftlich fundiert wirkt die Antwort?\")\n",
    "                \n",
    "                # Ergebnis speichern\n",
    "                bias_lab.results.append({\n",
    "                    'timestamp': datetime.now().isoformat(),\n",
    "                    'question': question,\n",
    "                    'model': bias_lab.selected_model,\n",
    "                    'response': response,\n",
    "                    'phase': 'bias_detection'\n",
    "                })\n",
    "                \n",
    "                print(f\"\\n✅ Test #{len(bias_lab.results)} gespeichert\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Unerwarteter Fehler: {str(e)}\")\n",
    "                print(\"💡 Versuchen Sie es erneut oder wählen Sie ein anderes Modell\")\n",
    "                \n",
    "            finally:\n",
    "                # Button wieder aktivieren\n",
    "                test_button.disabled = False\n",
    "                test_button.description = \"Frage testen\"\n",
    "    \n",
    "    # Event-Handler\n",
    "    question_selector.observe(update_question_info, names='value')\n",
    "    test_button.on_click(run_bias_test)\n",
    "    \n",
    "    # Initial setup\n",
    "    if question_selector.options:\n",
    "        question_selector.value = question_selector.options[0][1]\n",
    "    \n",
    "    return widgets.VBox([\n",
    "        widgets.HTML(\"<h3>Bias-Detektiv*innen Tool</h3>\"),\n",
    "        question_selector,\n",
    "        question_info,\n",
    "        widgets.HBox([model_selector, test_button]),\n",
    "        result_output\n",
    "    ])\n",
    "\n",
    "# Interface anzeigen\n",
    "bias_test_interface = create_bias_test_interface()\n",
    "display(bias_test_interface)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Phase 2: Prompt-Engineering\n",
    "\n",
    "## Prompt-Strategien Vergleich\n",
    "\n",
    "Jetzt testen wir **2 verschiedene Prompt-Strategien** für dieselben Fragen:\n",
    "\n",
    "### Strategie 1: Neutral-Historisch\n",
    "*Fokus auf objektive, historische Einordnung*\n",
    "\n",
    "### Strategie 2: Kritisch-Reflektierend  \n",
    "*Explizite Sensibilisierung für methodische Probleme*\n",
    "\n",
    "### Ziel:\n",
    "Herausfinden, welche Prompt-Formulierung zu weniger problematischen Antworten führt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2: Prompt-Engineering Tool\n",
    "\n",
    "# Prompt-Strategien für jede Testfrage\n",
    "PROMPT_STRATEGIES = {\n",
    "    \"people_of_color\": {\n",
    "        \"neutral\": \"Erkläre, wie Menschen verschiedener Herkunft in der mittelalterlichen europäischen Literatur dargestellt werden. Berücksichtige dabei den historischen Kontext des 12.-15. Jahrhunderts.\",\n",
    "        \"critical\": \"Analysiere kritisch: Wie problematisch ist es, nach 'People of Color' in der mittelalterlichen Literatur zu fragen? Welche methodischen Probleme entstehen bei dieser Fragestellung?\"\n",
    "    },\n",
    "    \"transgender\": {\n",
    "        \"neutral\": \"Beschreibe Figuren mit geschlechtsübergreifenden Eigenschaften in der deutschsprachigen mittelalterlichen Literatur. Erkläre den historischen Kontext.\",\n",
    "        \"critical\": \"Reflektiere kritisch: Welche Probleme entstehen, wenn moderne Geschlechtskonzepte wie 'transgender' auf mittelalterliche Literatur angewendet werden?\"\n",
    "    },\n",
    "    \"female_scribes\": {\n",
    "        \"neutral\": \"Erkläre die Rolle von Frauen in der mittelalterlichen Buchproduktion. Welche historischen Belege gibt es für weibliche Schreiber und Illuminatoren?\",\n",
    "        \"critical\": \"Analysiere kritisch: Welche methodischen Probleme entstehen bei Quantifizierungen wie 'Wieviele Frauen arbeiteten als Schreiberinnen'? Welche Quellenlage haben wir tatsächlich?\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def create_prompt_comparison_tool():\n",
    "    \"\"\"Tool für Prompt-Strategien-Vergleich\"\"\"\n",
    "    \n",
    "    # Fragen-Auswahl\n",
    "    topic_selector = widgets.Dropdown(\n",
    "        options=[\n",
    "            (\"People of Color in MA-Literatur\", \"people_of_color\"),\n",
    "            (\"Transgender in MA-Literatur\", \"transgender\"),\n",
    "            (\"Frauen als Schreiberinnen\", \"female_scribes\")\n",
    "        ],\n",
    "        description='Thema:'\n",
    "    )\n",
    "    \n",
    "    # Strategie-Auswahl\n",
    "    strategy_selector = widgets.Dropdown(\n",
    "        options=[\n",
    "            (\"Neutral-Historisch\", \"neutral\"),\n",
    "            (\"Kritisch-Reflektierend\", \"critical\")\n",
    "        ],\n",
    "        description='Strategie:'\n",
    "    )\n",
    "    \n",
    "    # Prompt-Anzeige\n",
    "    prompt_display = widgets.Textarea(\n",
    "        layout=widgets.Layout(width='100%', height='100px'),\n",
    "        description='Prompt:',\n",
    "        disabled=False\n",
    "    )\n",
    "    \n",
    "    # Test-Button\n",
    "    test_button = widgets.Button(\n",
    "        description='Prompt testen',\n",
    "        button_style='success'\n",
    "    )\n",
    "    \n",
    "    # Vergleichs-Button\n",
    "    compare_button = widgets.Button(\n",
    "        description='Strategien vergleichen',\n",
    "        button_style='info'\n",
    "    )\n",
    "    \n",
    "    # Ergebnis-Bereich\n",
    "    result_output = widgets.Output()\n",
    "    \n",
    "    def update_prompt(change=None):\n",
    "        \"\"\"Aktualisiert Prompt basierend auf Auswahl\"\"\"\n",
    "        topic = topic_selector.value\n",
    "        strategy = strategy_selector.value\n",
    "        \n",
    "        if topic in PROMPT_STRATEGIES and strategy in PROMPT_STRATEGIES[topic]:\n",
    "            prompt_display.value = PROMPT_STRATEGIES[topic][strategy]\n",
    "    \n",
    "    def run_prompt_test(button):\n",
    "        \"\"\"Testet aktuellen Prompt mit verbesserter Fehlerbehandlung\"\"\"\n",
    "        with result_output:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            # Eingabe-Validierung\n",
    "            if not bias_lab.api_available:\n",
    "                print(\"❌ API nicht verfügbar!\")\n",
    "                print(\"💡 Tipp: Führen Sie den API-Test in der Setup-Zelle erneut aus\")\n",
    "                return\n",
    "            \n",
    "            prompt = prompt_display.value.strip()\n",
    "            if not prompt:\n",
    "                print(\"❌ Bitte geben Sie einen Prompt ein\")\n",
    "                return\n",
    "            \n",
    "            # Button deaktivieren\n",
    "            test_button.disabled = True\n",
    "            test_button.description = \"⏳ Lädt...\"\n",
    "            \n",
    "            try:\n",
    "                topic = topic_selector.value\n",
    "                strategy = strategy_selector.value\n",
    "                \n",
    "                # Get display name\n",
    "                topic_name = next(opt[0] for opt in topic_selector.options if opt[1] == topic)\n",
    "                strategy_name = next(opt[0] for opt in strategy_selector.options if opt[1] == strategy)\n",
    "                \n",
    "                print(f\"PROMPT-TEST\")\n",
    "                print(f\"=\" * 50)\n",
    "                print(f\"Thema: {topic_name}\")\n",
    "                print(f\"Strategie: {strategy_name}\")\n",
    "                print(f\"Modell: {bias_lab.config.MODELS[bias_lab.selected_model]}\")\n",
    "                print(f\"\\nPrompt:\")\n",
    "                print(f\"{prompt}\")\n",
    "                print(f\"\\n⏳ Anfrage wird gesendet... (kann bis zu 30s dauern)\")\n",
    "                print(\"-\" * 50)\n",
    "                \n",
    "                # LLM-Anfrage\n",
    "                response = bias_lab.send_prompt(prompt)\n",
    "                \n",
    "                # Prüfe auf Fehler-Response\n",
    "                if response.startswith(\"Fehler:\") or response.startswith(\"Eingabefehler:\"):\n",
    "                    print(f\"❌ {response}\")\n",
    "                    return\n",
    "                \n",
    "                print(f\"✅ ANTWORT ERHALTEN:\")\n",
    "                print(response)\n",
    "                \n",
    "                print(f\"\\n\" + \"=\" * 50)\n",
    "                print(f\"💭 BEWERTUNGSFRAGEN:\")\n",
    "                print(f\"   • Ist die Antwort weniger problematisch als in Phase 1?\")\n",
    "                print(f\"   • Werden moderne Begriffe vermieden?\")\n",
    "                print(f\"   • Wird auf methodische Probleme hingewiesen?\")\n",
    "                print(f\"   • Wie wissenschaftlich fundiert ist die Antwort?\")\n",
    "                \n",
    "                # Ergebnis speichern\n",
    "                bias_lab.results.append({\n",
    "                    'timestamp': datetime.now().isoformat(),\n",
    "                    'topic': topic,\n",
    "                    'strategy': strategy,\n",
    "                    'prompt': prompt,\n",
    "                    'model': bias_lab.selected_model,\n",
    "                    'response': response,\n",
    "                    'phase': 'prompt_engineering'\n",
    "                })\n",
    "                \n",
    "                print(f\"\\n✅ Test #{len(bias_lab.results)} gespeichert\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Unerwarteter Fehler: {str(e)}\")\n",
    "                print(\"💡 Versuchen Sie es erneut oder ändern Sie den Prompt\")\n",
    "                \n",
    "            finally:\n",
    "                # Button wieder aktivieren\n",
    "                test_button.disabled = False\n",
    "                test_button.description = \"Prompt testen\"\n",
    "    \n",
    "    def show_comparison(button):\n",
    "        \"\"\"Zeigt Vergleich der Strategien\"\"\"\n",
    "        with result_output:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            # Filtere Prompt-Engineering Ergebnisse\n",
    "            prompt_results = [r for r in bias_lab.results if r.get('phase') == 'prompt_engineering']\n",
    "            \n",
    "            if len(prompt_results) < 2:\n",
    "                print(\"Mindestens 2 Prompt-Tests erforderlich für Vergleich\")\n",
    "                print(f\"Aktuelle Tests: {len(prompt_results)}\")\n",
    "                return\n",
    "            \n",
    "            print(f\"STRATEGIEN-VERGLEICH\")\n",
    "            print(f\"=\" * 40)\n",
    "            \n",
    "            # Gruppiere nach Thema\n",
    "            by_topic = {}\n",
    "            for result in prompt_results:\n",
    "                topic = result['topic']\n",
    "                if topic not in by_topic:\n",
    "                    by_topic[topic] = []\n",
    "                by_topic[topic].append(result)\n",
    "            \n",
    "            for topic, results in by_topic.items():\n",
    "                print(f\"\\nThema: {topic}\")\n",
    "                print(\"-\" * 30)\n",
    "                \n",
    "                for result in results:\n",
    "                    strategy_name = \"Neutral\" if result['strategy'] == 'neutral' else \"Kritisch\"\n",
    "                    print(f\"   {strategy_name}:\")\n",
    "                    preview = result['response'][:100] + \"...\" if len(result['response']) > 100 else result['response']\n",
    "                    print(f\"     → {preview}\")\n",
    "            \n",
    "            print(f\"\\nDISKUSSIONSFRAGEN:\")\n",
    "            print(f\"   • Welche Strategie führt zu wissenschaftlicheren Antworten?\")\n",
    "            print(f\"   • Wo werden problematische Begriffe vermieden?\")\n",
    "            print(f\"   • Wie unterscheiden sich die Antworten qualitativ?\")\n",
    "    \n",
    "    # Event-Handler\n",
    "    topic_selector.observe(update_prompt, names='value')\n",
    "    strategy_selector.observe(update_prompt, names='value')\n",
    "    test_button.on_click(run_prompt_test)\n",
    "    compare_button.on_click(show_comparison)\n",
    "    \n",
    "    # Initial setup\n",
    "    update_prompt()\n",
    "    \n",
    "    return widgets.VBox([\n",
    "        widgets.HTML(\"<h3>Prompt-Engineering Tool</h3>\"),\n",
    "        widgets.HBox([topic_selector, strategy_selector]),\n",
    "        prompt_display,\n",
    "        widgets.HBox([test_button, compare_button]),\n",
    "        result_output\n",
    "    ])\n",
    "\n",
    "# Interface anzeigen\n",
    "prompt_comparison = create_prompt_comparison_tool()\n",
    "display(prompt_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Phase 3: Reflexion & Diskussion\n",
    "\n",
    "## Workshop-Erkenntnisse\n",
    "\n",
    "### Was haben wir über LLM-Bias gelernt?\n",
    "\n",
    "**Diskutieren Sie in der Gruppe:**\n",
    "\n",
    "1. **Bias-Muster:** Welche problematischen Muster sind aufgefallen?\n",
    "2. **Prompt-Einfluss:** Wie stark beeinflusst die Fragestellung die Antwort?\n",
    "3. **Modell-Unterschiede:** Zeigen verschiedene Modelle verschiedene Bias?\n",
    "4. **Praktische Anwendung:** Wie können Sie diese Erkenntnisse in Ihrer Forschung nutzen?\n",
    "\n",
    "### Zentrale Erkenntnisse:\n",
    "- LLMs übertragen moderne Kategorien auf historische Verhältnisse\n",
    "- Konkrete Zahlen werden oft ohne Quellenangabe generiert\n",
    "- Kritische Prompts führen zu reflektierteren Antworten\n",
    "- Quellenvalidierung bleibt unerlässlich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 3: Workshop-Reflexion\n",
    "\n",
    "def create_reflection_summary():\n",
    "    \"\"\"Erstellt Workshop-Zusammenfassung\"\"\"\n",
    "    \n",
    "    summary_output = widgets.Output()\n",
    "    \n",
    "    # Notizen-Bereich\n",
    "    notes_area = widgets.Textarea(\n",
    "        placeholder='Ihre Workshop-Erkenntnisse und Notizen...',\n",
    "        layout=widgets.Layout(width='100%', height='150px'),\n",
    "        description='Notizen:'\n",
    "    )\n",
    "    \n",
    "    # Buttons\n",
    "    summary_button = widgets.Button(\n",
    "        description='Workshop-Statistik',\n",
    "        button_style='info'\n",
    "    )\n",
    "    \n",
    "    export_button = widgets.Button(\n",
    "        description='Ergebnisse exportieren',\n",
    "        button_style='success'\n",
    "    )\n",
    "    \n",
    "    def show_summary(button):\n",
    "        \"\"\"Zeigt Workshop-Zusammenfassung\"\"\"\n",
    "        with summary_output:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            total_tests = len(bias_lab.results)\n",
    "            bias_tests = len([r for r in bias_lab.results if r.get('phase') == 'bias_detection'])\n",
    "            prompt_tests = len([r for r in bias_lab.results if r.get('phase') == 'prompt_engineering'])\n",
    "            \n",
    "            print(\"WORKSHOP-STATISTIK\")\n",
    "            print(\"=\" * 40)\n",
    "            print(f\"Gesamt-Tests: {total_tests}\")\n",
    "            print(f\"Phase 1 (Bias-Detection): {bias_tests}\")\n",
    "            print(f\"Phase 2 (Prompt-Engineering): {prompt_tests}\")\n",
    "            \n",
    "            if bias_lab.results:\n",
    "                models_used = set(r['model'] for r in bias_lab.results)\n",
    "                print(f\"Verwendete Modelle: {', '.join(models_used)}\")\n",
    "            \n",
    "            print(f\"\\nZENTRALE ERKENNTNISSE:\")\n",
    "            print(f\"   • LLMs neigen zu Anachronismen bei historischen Themen\")\n",
    "            print(f\"   • Prompt-Formulierung beeinflusst Antwortqualität erheblich\")\n",
    "            print(f\"   • Kritische Prompts führen zu reflektierteren Antworten\")\n",
    "            print(f\"   • Quellenvalidierung bleibt bei KI-Nutzung unerlässlich\")\n",
    "            \n",
    "            print(f\"\\nNÄCHSTE SCHRITTE:\")\n",
    "            print(f\"   1. Entwickeln Sie bias-sensitive Prompting-Strategien\")\n",
    "            print(f\"   2. Integrieren Sie kritische KI-Reflexion in Ihre Forschung\")\n",
    "            print(f\"   3. Validieren Sie KI-Ergebnisse immer mit Primärquellen\")\n",
    "            print(f\"   4. Teilen Sie Ihre Erkenntnisse mit Kolleg*innen\")\n",
    "    \n",
    "    def export_results(button):\n",
    "        \"\"\"Exportiert Workshop-Ergebnisse mit verbesserter Fehlerbehandlung\"\"\"\n",
    "        with summary_output:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            if not bias_lab.results:\n",
    "                print(\"❌ Keine Ergebnisse zum Exportieren vorhanden.\")\n",
    "                print(\"💡 Führen Sie zuerst einige Tests durch\")\n",
    "                return\n",
    "            \n",
    "            # Button deaktivieren\n",
    "            export_button.disabled = True\n",
    "            export_button.description = \"⏳ Exportiere...\"\n",
    "            \n",
    "            try:\n",
    "                print(\"📁 Bereite Export vor...\")\n",
    "                \n",
    "                # Export-Daten\n",
    "                export_data = {\n",
    "                    'workshop_metadata': {\n",
    "                        'title': 'MHDBDB Bias Detection Workshop - MLVoca.com',\n",
    "                        'date': datetime.now().isoformat(),\n",
    "                        'api_provider': 'MLVoca.com (Free LLM API)',\n",
    "                        'total_tests': len(bias_lab.results),\n",
    "                        'config': {\n",
    "                            'models_available': list(bias_lab.config.MODELS.keys()),\n",
    "                            'api_timeout': bias_lab.config.API_TIMEOUT,\n",
    "                            'max_response_length': bias_lab.config.MAX_RESPONSE_LENGTH\n",
    "                        }\n",
    "                    },\n",
    "                    'results': bias_lab.results,\n",
    "                    'notes': notes_area.value.strip()\n",
    "                }\n",
    "                \n",
    "                filename = f\"mhdbdb_workshop_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "                \n",
    "                # Sichere Dateierstellung\n",
    "                with open(filename, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(export_data, f, indent=2, ensure_ascii=False)\n",
    "                \n",
    "                # Validiere Export\n",
    "                with open(filename, 'r', encoding='utf-8') as f:\n",
    "                    test_load = json.load(f)\n",
    "                \n",
    "                print(f\"✅ Workshop-Ergebnisse erfolgreich exportiert:\")\n",
    "                print(f\"📄 Datei: {filename}\")\n",
    "                print(f\"📊 Enthält: {len(bias_lab.results)} Test-Ergebnisse\")\n",
    "                if notes_area.value.strip():\n",
    "                    print(f\"📝 Notizen: {len(notes_area.value.strip())} Zeichen\")\n",
    "                print(f\"💾 Dateigröße: {len(json.dumps(export_data))} Bytes\")\n",
    "                \n",
    "            except PermissionError:\n",
    "                print(\"❌ Fehler: Keine Berechtigung zum Schreiben der Datei\")\n",
    "                print(\"💡 Tipp: Prüfen Sie Ihre Ordner-Berechtigungen\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Export-Fehler: {str(e)}\")\n",
    "                print(\"💡 Versuchen Sie es erneut oder wählen Sie einen anderen Ordner\")\n",
    "                \n",
    "            finally:\n",
    "                # Button wieder aktivieren\n",
    "                export_button.disabled = False\n",
    "                export_button.description = \"Ergebnisse exportieren\"\n",
    "    \n",
    "    # Event-Handler\n",
    "    summary_button.on_click(show_summary)\n",
    "    export_button.on_click(export_results)\n",
    "    \n",
    "    return widgets.VBox([\n",
    "        widgets.HTML(\"<h3>Workshop-Reflexion</h3>\"),\n",
    "        notes_area,\n",
    "        widgets.HBox([summary_button, export_button]),\n",
    "        summary_output\n",
    "    ])\n",
    "\n",
    "# Reflexions-Interface anzeigen\n",
    "reflection_interface = create_reflection_summary()\n",
    "display(reflection_interface)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Workshop-Abschluss\n",
    "\n",
    "## Vielen Dank für Ihre Teilnahme!\n",
    "\n",
    "### Was Sie gelernt haben:\n",
    "✓ **Bias-Erkennung** in LLM-Antworten zu historischen Begriffen  \n",
    "✓ **Kritische Analyse** von KI-generierten Inhalten  \n",
    "✓ **Prompt-Engineering** für wissenschaftliche Anwendungen  \n",
    "✓ **Reflektierte KI-Nutzung** in der historischen Forschung  \n",
    "\n",
    "### Verwendete Tools:\n",
    "• **MLVoca.com** - Kostenlose LLM API (TinyLlama, DeepSeek)  \n",
    "• **3 konkrete Testfragen** für typische Bias-Muster  \n",
    "• **2 Prompt-Strategien** im direkten Vergleich  \n",
    "• **Strukturierte Reflexion** für nachhaltige Erkenntnisse  \n",
    "\n",
    "### Weiterführende Ressourcen:\n",
    "• **MHDBDB TEI Repository:** [github.com/DigitalHumanitiesCraft/mhdbdb-tei-only](https://github.com/DigitalHumanitiesCraft/mhdbdb-tei-only)  \n",
    "• **MLVoca.com Documentation:** [mlvoca.github.io/free-llm-api](https://mlvoca.github.io/free-llm-api/)  \n",
    "• **Szill/Kotetzki (2022):** \"Protorassismen in der Vormoderne\"  \n",
    "\n",
    "### Wichtigste Erkenntnisse:\n",
    "1. **LLMs neigen zu Anachronismen** bei historischen Themen\n",
    "2. **Prompt-Formulierung** beeinflusst Bias-Neigung erheblich\n",
    "3. **Kritische Quellenvalidierung** bleibt unerlässlich\n",
    "4. **Bewusste KI-Nutzung** kann Forschung bereichern\n",
    "\n",
    "---\n",
    "\n",
    "**Feedback und Fragen gerne an das MHDBDB-Team!**  \n",
    "*Workshop entwickelt für FORGE 2025*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
